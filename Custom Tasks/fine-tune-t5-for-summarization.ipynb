{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import cuda\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Inshorts Cleaned Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Short</th>\n",
       "      <th>Source</th>\n",
       "      <th>Time</th>\n",
       "      <th>Publish Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
       "      <td>The CBI on Saturday booked four former officia...</td>\n",
       "      <td>The New Indian Express</td>\n",
       "      <td>09:25:00</td>\n",
       "      <td>2017-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
       "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
       "      <td>Outlook</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
       "      <td>At least three people were killed, including a...</td>\n",
       "      <td>Hindustan Times</td>\n",
       "      <td>23:39:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why has Reliance been barred from trading in f...</td>\n",
       "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>23:08:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Was stopped from entering my own studio at Tim...</td>\n",
       "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
       "      <td>YouTube</td>\n",
       "      <td>23:24:00</td>\n",
       "      <td>2017-03-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  4 ex-bank officials booked for cheating bank o...   \n",
       "1     Supreme Court to go paperless in 6 months: CJI   \n",
       "2  At least 3 killed, 30 injured in blast in Sylh...   \n",
       "3  Why has Reliance been barred from trading in f...   \n",
       "4  Was stopped from entering my own studio at Tim...   \n",
       "\n",
       "                                               Short                 Source   \\\n",
       "0  The CBI on Saturday booked four former officia...  The New Indian Express   \n",
       "1  Chief Justice JS Khehar has said the Supreme C...                 Outlook   \n",
       "2  At least three people were killed, including a...         Hindustan Times   \n",
       "3  Mukesh Ambani-led Reliance Industries (RIL) wa...                Livemint   \n",
       "4  TV news anchor Arnab Goswami has said he was t...                 YouTube   \n",
       "\n",
       "      Time  Publish Date  \n",
       "0  09:25:00   2017-03-26  \n",
       "1  22:18:00   2017-03-25  \n",
       "2  23:39:00   2017-03-25  \n",
       "3  23:08:00   2017-03-25  \n",
       "4  23:24:00   2017-03-25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.Headline\n",
    "        self.ctext = self.data.Short\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length = self.source_len, padding = 'max_length',\\\n",
    "                                                  return_tensors = 'pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length = self.summ_len, padding = 'max_length',\\\n",
    "                                                  return_tensors = 'pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype = torch.long), \n",
    "            'source_mask': source_mask.to(dtype = torch.long), \n",
    "            'target_ids': target_ids.to(dtype = torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous() # make y_ids contiguous \n",
    "        lm_labels = y[:, 1:].clone().detach() # make fast copy\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 # replace pad tokens \n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids = y_ids, labels = lm_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length = 100, \n",
    "                num_beams = 2,\n",
    "                repetition_penalty = 2.5, \n",
    "                length_penalty = 1.0, \n",
    "                early_stopping = True\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens = True, clean_up_tokenization_spaces = True)\\\n",
    "                     for g in generated_ids]\n",
    "            target = [tokenizer.decode(t, skip_special_tokens = True, clean_up_tokenization_spaces = True)\\\n",
    "                      for t in y]\n",
    "            if _%100==0:\n",
    "                print(f'Completed {_}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (55104, 2)\n",
      "TRAIN Dataset: (44083, 2)\n",
      "TEST Dataset: (11021, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at t5-base were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 2\n",
    "VALID_BATCH_SIZE = 2 \n",
    "TRAIN_EPOCHS = 10      \n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    \n",
    "SEED = 42               \n",
    "MAX_LEN = 512\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "torch.manual_seed(SEED) \n",
    "np.random.seed(SEED) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "df = df[['Headline','Short']]\n",
    "df.Headline = 'summarize: ' + df.Headline\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size,random_state = SEED)\n",
    "val_dataset = df.drop(train_dataset.index).reset_index(drop = True)\n",
    "train_dataset = train_dataset.reset_index(drop = True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating Fine-Tuning for the model on our dataset\n",
      "Epoch: 0, Loss:  5.8626837730407715\n",
      "Epoch: 0, Loss:  1.535601019859314\n",
      "Epoch: 0, Loss:  1.0182313919067383\n",
      "Epoch: 0, Loss:  2.2324159145355225\n",
      "Epoch: 0, Loss:  1.2453153133392334\n",
      "Epoch: 0, Loss:  1.2783634662628174\n",
      "Epoch: 0, Loss:  2.4592106342315674\n",
      "Epoch: 0, Loss:  0.6765856742858887\n",
      "Epoch: 0, Loss:  1.6162558794021606\n",
      "Epoch: 0, Loss:  0.9466649293899536\n",
      "Epoch: 0, Loss:  1.051295280456543\n",
      "Epoch: 0, Loss:  0.5608026385307312\n",
      "Epoch: 0, Loss:  1.1044048070907593\n",
      "Epoch: 0, Loss:  0.6694570779800415\n",
      "Epoch: 0, Loss:  0.8327499032020569\n",
      "Epoch: 0, Loss:  0.9408615231513977\n",
      "Epoch: 0, Loss:  0.5147339701652527\n",
      "Epoch: 0, Loss:  0.9305832982063293\n",
      "Epoch: 0, Loss:  1.420927882194519\n",
      "Epoch: 0, Loss:  1.6219096183776855\n",
      "Epoch: 0, Loss:  0.4562230110168457\n",
      "Epoch: 0, Loss:  1.4659337997436523\n",
      "Epoch: 0, Loss:  1.1376267671585083\n",
      "Epoch: 0, Loss:  1.4021466970443726\n",
      "Epoch: 0, Loss:  0.871911883354187\n",
      "Epoch: 0, Loss:  0.48270684480667114\n",
      "Epoch: 0, Loss:  1.092772364616394\n",
      "Epoch: 0, Loss:  0.8125654458999634\n",
      "Epoch: 0, Loss:  1.4696111679077148\n",
      "Epoch: 0, Loss:  1.7436566352844238\n",
      "Epoch: 0, Loss:  1.0834366083145142\n",
      "Epoch: 0, Loss:  1.2271381616592407\n",
      "Epoch: 0, Loss:  1.2535463571548462\n",
      "Epoch: 0, Loss:  1.9159594774246216\n",
      "Epoch: 0, Loss:  1.2059776782989502\n",
      "Epoch: 0, Loss:  0.863342821598053\n",
      "Epoch: 0, Loss:  1.5719765424728394\n",
      "Epoch: 0, Loss:  0.6642292737960815\n",
      "Epoch: 0, Loss:  1.121624231338501\n",
      "Epoch: 0, Loss:  0.9437480568885803\n",
      "Epoch: 0, Loss:  0.973336935043335\n",
      "Epoch: 0, Loss:  1.1642489433288574\n",
      "Epoch: 0, Loss:  0.7758702039718628\n",
      "Epoch: 0, Loss:  0.6798349618911743\n",
      "Epoch: 0, Loss:  0.44392484426498413\n",
      "Epoch: 1, Loss:  1.310034155845642\n",
      "Epoch: 1, Loss:  0.38525572419166565\n",
      "Epoch: 1, Loss:  1.196498990058899\n",
      "Epoch: 1, Loss:  1.1521135568618774\n",
      "Epoch: 1, Loss:  0.8019075393676758\n",
      "Epoch: 1, Loss:  0.6093102693557739\n",
      "Epoch: 1, Loss:  0.594188392162323\n",
      "Epoch: 1, Loss:  0.8247729539871216\n",
      "Epoch: 1, Loss:  1.113587737083435\n",
      "Epoch: 1, Loss:  1.212226152420044\n",
      "Epoch: 1, Loss:  0.5894128680229187\n",
      "Epoch: 1, Loss:  1.9138193130493164\n",
      "Epoch: 1, Loss:  0.6083487272262573\n",
      "Epoch: 1, Loss:  0.9107183814048767\n",
      "Epoch: 1, Loss:  0.8475868105888367\n",
      "Epoch: 1, Loss:  0.9773712158203125\n",
      "Epoch: 1, Loss:  1.1908271312713623\n",
      "Epoch: 1, Loss:  1.093553066253662\n",
      "Epoch: 1, Loss:  1.721745491027832\n",
      "Epoch: 1, Loss:  1.0379806756973267\n",
      "Epoch: 1, Loss:  1.2404654026031494\n",
      "Epoch: 1, Loss:  1.2371771335601807\n",
      "Epoch: 1, Loss:  0.7332926988601685\n",
      "Epoch: 1, Loss:  1.7264623641967773\n",
      "Epoch: 1, Loss:  0.8828302025794983\n",
      "Epoch: 1, Loss:  1.2844135761260986\n",
      "Epoch: 1, Loss:  1.1027717590332031\n",
      "Epoch: 1, Loss:  0.9434088468551636\n",
      "Epoch: 1, Loss:  0.36587169766426086\n",
      "Epoch: 1, Loss:  1.2062416076660156\n",
      "Epoch: 1, Loss:  0.955967903137207\n",
      "Epoch: 1, Loss:  1.3125560283660889\n",
      "Epoch: 1, Loss:  1.1526470184326172\n",
      "Epoch: 1, Loss:  0.6526896357536316\n",
      "Epoch: 1, Loss:  1.383000373840332\n",
      "Epoch: 1, Loss:  1.2594696283340454\n",
      "Epoch: 1, Loss:  1.313788890838623\n",
      "Epoch: 1, Loss:  0.4873969852924347\n",
      "Epoch: 1, Loss:  1.1015032529830933\n",
      "Epoch: 1, Loss:  1.673604130744934\n",
      "Epoch: 1, Loss:  0.7583734393119812\n",
      "Epoch: 1, Loss:  0.8315441012382507\n",
      "Epoch: 1, Loss:  0.48001590371131897\n",
      "Epoch: 1, Loss:  0.7520274519920349\n",
      "Epoch: 1, Loss:  0.7584529519081116\n",
      "Epoch: 2, Loss:  0.8327208757400513\n",
      "Epoch: 2, Loss:  0.4398657977581024\n",
      "Epoch: 2, Loss:  0.4831171929836273\n",
      "Epoch: 2, Loss:  0.8464341163635254\n",
      "Epoch: 2, Loss:  0.7691086530685425\n",
      "Epoch: 2, Loss:  0.8051785230636597\n",
      "Epoch: 2, Loss:  0.4463484585285187\n",
      "Epoch: 2, Loss:  0.9687475562095642\n",
      "Epoch: 2, Loss:  1.01504385471344\n",
      "Epoch: 2, Loss:  1.0057792663574219\n",
      "Epoch: 2, Loss:  1.091397762298584\n",
      "Epoch: 2, Loss:  1.334436297416687\n",
      "Epoch: 2, Loss:  0.7578896880149841\n",
      "Epoch: 2, Loss:  0.2800973653793335\n",
      "Epoch: 2, Loss:  1.2051275968551636\n",
      "Epoch: 2, Loss:  0.6153272986412048\n",
      "Epoch: 2, Loss:  0.5795153379440308\n",
      "Epoch: 2, Loss:  0.7675348520278931\n",
      "Epoch: 2, Loss:  1.1532397270202637\n",
      "Epoch: 2, Loss:  1.4554275274276733\n",
      "Epoch: 2, Loss:  0.722303032875061\n",
      "Epoch: 2, Loss:  1.1139343976974487\n",
      "Epoch: 2, Loss:  0.5963853597640991\n",
      "Epoch: 2, Loss:  1.1829018592834473\n",
      "Epoch: 2, Loss:  1.0995279550552368\n",
      "Epoch: 2, Loss:  1.0524225234985352\n",
      "Epoch: 2, Loss:  1.1202119588851929\n",
      "Epoch: 2, Loss:  0.8344758749008179\n",
      "Epoch: 2, Loss:  0.4640265107154846\n",
      "Epoch: 2, Loss:  0.8298894166946411\n",
      "Epoch: 2, Loss:  1.002247929573059\n",
      "Epoch: 2, Loss:  0.6233884692192078\n",
      "Epoch: 2, Loss:  1.1812570095062256\n",
      "Epoch: 2, Loss:  1.5957738161087036\n",
      "Epoch: 2, Loss:  1.0486465692520142\n",
      "Epoch: 2, Loss:  0.8198096752166748\n",
      "Epoch: 2, Loss:  0.49948379397392273\n",
      "Epoch: 2, Loss:  1.0656800270080566\n",
      "Epoch: 2, Loss:  0.9998241066932678\n",
      "Epoch: 2, Loss:  0.9609076976776123\n",
      "Epoch: 2, Loss:  0.7530308961868286\n",
      "Epoch: 2, Loss:  0.2635551989078522\n",
      "Epoch: 2, Loss:  0.6395062804222107\n",
      "Epoch: 2, Loss:  1.0542232990264893\n",
      "Epoch: 2, Loss:  0.3025949001312256\n",
      "Epoch: 3, Loss:  0.6394652128219604\n",
      "Epoch: 3, Loss:  0.481310099363327\n",
      "Epoch: 3, Loss:  0.7489912509918213\n",
      "Epoch: 3, Loss:  0.5009312629699707\n",
      "Epoch: 3, Loss:  0.5715447664260864\n",
      "Epoch: 3, Loss:  0.745578944683075\n",
      "Epoch: 3, Loss:  0.2409784197807312\n",
      "Epoch: 3, Loss:  0.9606683254241943\n",
      "Epoch: 3, Loss:  1.0687413215637207\n",
      "Epoch: 3, Loss:  0.19928540289402008\n",
      "Epoch: 3, Loss:  0.3630232512950897\n",
      "Epoch: 3, Loss:  0.8652393221855164\n",
      "Epoch: 3, Loss:  0.7098144888877869\n",
      "Epoch: 3, Loss:  0.443859726190567\n",
      "Epoch: 3, Loss:  1.2494632005691528\n",
      "Epoch: 3, Loss:  0.23644623160362244\n",
      "Epoch: 3, Loss:  0.547264039516449\n",
      "Epoch: 3, Loss:  0.8627164959907532\n",
      "Epoch: 3, Loss:  0.7747825384140015\n",
      "Epoch: 3, Loss:  0.6602984070777893\n",
      "Epoch: 3, Loss:  0.771390438079834\n",
      "Epoch: 3, Loss:  0.6374475955963135\n",
      "Epoch: 3, Loss:  0.6287018656730652\n",
      "Epoch: 3, Loss:  0.9104929566383362\n",
      "Epoch: 3, Loss:  0.7298685312271118\n",
      "Epoch: 3, Loss:  0.7368289828300476\n",
      "Epoch: 3, Loss:  0.6688300371170044\n",
      "Epoch: 3, Loss:  0.5257796049118042\n",
      "Epoch: 3, Loss:  0.4744293987751007\n",
      "Epoch: 3, Loss:  0.6725660562515259\n",
      "Epoch: 3, Loss:  1.1121336221694946\n",
      "Epoch: 3, Loss:  0.5033054351806641\n",
      "Epoch: 3, Loss:  0.5019561648368835\n",
      "Epoch: 3, Loss:  0.9649445414543152\n",
      "Epoch: 3, Loss:  0.24874930083751678\n",
      "Epoch: 3, Loss:  0.38905978202819824\n",
      "Epoch: 3, Loss:  0.8164553046226501\n",
      "Epoch: 3, Loss:  0.9246434569358826\n",
      "Epoch: 3, Loss:  1.000304102897644\n",
      "Epoch: 3, Loss:  0.25181955099105835\n",
      "Epoch: 3, Loss:  0.7740283608436584\n",
      "Epoch: 3, Loss:  0.47156810760498047\n",
      "Epoch: 3, Loss:  0.6196098327636719\n",
      "Epoch: 3, Loss:  0.9406095147132874\n",
      "Epoch: 3, Loss:  0.3608807921409607\n",
      "Epoch: 4, Loss:  1.3803553581237793\n",
      "Epoch: 4, Loss:  0.5034658908843994\n",
      "Epoch: 4, Loss:  0.4790048599243164\n",
      "Epoch: 4, Loss:  0.6742090582847595\n",
      "Epoch: 4, Loss:  0.9742489457130432\n",
      "Epoch: 4, Loss:  0.36010414361953735\n",
      "Epoch: 4, Loss:  0.6873635053634644\n",
      "Epoch: 4, Loss:  0.38448020815849304\n",
      "Epoch: 4, Loss:  0.28499579429626465\n",
      "Epoch: 4, Loss:  0.5580074191093445\n",
      "Epoch: 4, Loss:  0.3369782269001007\n",
      "Epoch: 4, Loss:  0.7334256768226624\n",
      "Epoch: 4, Loss:  0.38217195868492126\n",
      "Epoch: 4, Loss:  0.4246302843093872\n",
      "Epoch: 4, Loss:  0.2003067433834076\n",
      "Epoch: 4, Loss:  0.21749576926231384\n",
      "Epoch: 4, Loss:  0.542262852191925\n",
      "Epoch: 4, Loss:  0.7604562640190125\n",
      "Epoch: 4, Loss:  1.2093323469161987\n",
      "Epoch: 4, Loss:  1.078662395477295\n",
      "Epoch: 4, Loss:  0.15189166367053986\n",
      "Epoch: 4, Loss:  0.7006117105484009\n",
      "Epoch: 4, Loss:  0.3055388033390045\n",
      "Epoch: 4, Loss:  0.4975306987762451\n",
      "Epoch: 4, Loss:  0.3751193583011627\n",
      "Epoch: 4, Loss:  0.3123421370983124\n",
      "Epoch: 4, Loss:  0.3005853593349457\n",
      "Epoch: 4, Loss:  0.5959379076957703\n",
      "Epoch: 4, Loss:  0.700040876865387\n",
      "Epoch: 4, Loss:  0.47233185172080994\n",
      "Epoch: 4, Loss:  0.7553229331970215\n",
      "Epoch: 4, Loss:  0.7508143186569214\n",
      "Epoch: 4, Loss:  0.4695587158203125\n",
      "Epoch: 4, Loss:  0.4693911671638489\n",
      "Epoch: 4, Loss:  0.8867433667182922\n",
      "Epoch: 4, Loss:  0.5679031014442444\n",
      "Epoch: 4, Loss:  1.1471909284591675\n",
      "Epoch: 4, Loss:  0.3128097355365753\n",
      "Epoch: 4, Loss:  0.4117072820663452\n",
      "Epoch: 4, Loss:  0.45038914680480957\n",
      "Epoch: 4, Loss:  0.3399520218372345\n",
      "Epoch: 4, Loss:  0.2627584636211395\n",
      "Epoch: 4, Loss:  0.6539748311042786\n",
      "Epoch: 4, Loss:  0.5587921738624573\n",
      "Epoch: 4, Loss:  0.6404399871826172\n",
      "Epoch: 5, Loss:  0.6475744843482971\n",
      "Epoch: 5, Loss:  0.09841080754995346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.4171774089336395\n",
      "Epoch: 5, Loss:  0.2690272331237793\n",
      "Epoch: 5, Loss:  0.532575249671936\n",
      "Epoch: 5, Loss:  0.7844458222389221\n",
      "Epoch: 5, Loss:  0.3309836685657501\n",
      "Epoch: 5, Loss:  0.3058425486087799\n",
      "Epoch: 5, Loss:  0.6042974591255188\n",
      "Epoch: 5, Loss:  0.38814160227775574\n",
      "Epoch: 5, Loss:  0.5136160254478455\n",
      "Epoch: 5, Loss:  0.8710018992424011\n",
      "Epoch: 5, Loss:  0.5453805923461914\n",
      "Epoch: 5, Loss:  0.30934610962867737\n",
      "Epoch: 5, Loss:  0.7441872954368591\n",
      "Epoch: 5, Loss:  0.2547917068004608\n",
      "Epoch: 5, Loss:  0.4765646457672119\n",
      "Epoch: 5, Loss:  0.46921685338020325\n",
      "Epoch: 5, Loss:  0.31470802426338196\n",
      "Epoch: 5, Loss:  0.33740732073783875\n",
      "Epoch: 5, Loss:  0.482659250497818\n",
      "Epoch: 5, Loss:  0.34207069873809814\n",
      "Epoch: 5, Loss:  0.5052774548530579\n",
      "Epoch: 5, Loss:  0.4857509732246399\n",
      "Epoch: 5, Loss:  0.8999201655387878\n",
      "Epoch: 5, Loss:  0.47042861580848694\n",
      "Epoch: 5, Loss:  0.5252408385276794\n",
      "Epoch: 5, Loss:  0.3099709749221802\n",
      "Epoch: 5, Loss:  0.08584362268447876\n",
      "Epoch: 5, Loss:  0.7241501808166504\n",
      "Epoch: 5, Loss:  0.6318396925926208\n",
      "Epoch: 5, Loss:  0.4702340066432953\n",
      "Epoch: 5, Loss:  0.5783410668373108\n",
      "Epoch: 5, Loss:  0.3364514410495758\n",
      "Epoch: 5, Loss:  0.21366579830646515\n",
      "Epoch: 5, Loss:  0.5593158006668091\n",
      "Epoch: 5, Loss:  0.3920661211013794\n",
      "Epoch: 5, Loss:  0.3920026421546936\n",
      "Epoch: 5, Loss:  0.37654536962509155\n",
      "Epoch: 5, Loss:  0.13679471611976624\n",
      "Epoch: 5, Loss:  0.6522150039672852\n",
      "Epoch: 5, Loss:  0.31679829955101013\n",
      "Epoch: 5, Loss:  0.5570260286331177\n",
      "Epoch: 5, Loss:  0.6017898917198181\n",
      "Epoch: 5, Loss:  0.393054723739624\n",
      "Epoch: 6, Loss:  0.26535946130752563\n",
      "Epoch: 6, Loss:  0.49238771200180054\n",
      "Epoch: 6, Loss:  0.4695848822593689\n",
      "Epoch: 6, Loss:  0.2252381145954132\n",
      "Epoch: 6, Loss:  0.11704186350107193\n",
      "Epoch: 6, Loss:  0.10256937891244888\n",
      "Epoch: 6, Loss:  0.33766523003578186\n",
      "Epoch: 6, Loss:  0.6736148595809937\n",
      "Epoch: 6, Loss:  0.1502036452293396\n",
      "Epoch: 6, Loss:  0.16144108772277832\n",
      "Epoch: 6, Loss:  0.15599308907985687\n",
      "Epoch: 6, Loss:  0.35034722089767456\n",
      "Epoch: 6, Loss:  0.23975908756256104\n",
      "Epoch: 6, Loss:  0.32071202993392944\n",
      "Epoch: 6, Loss:  0.23673765361309052\n",
      "Epoch: 6, Loss:  0.7267942428588867\n",
      "Epoch: 6, Loss:  0.28239190578460693\n",
      "Epoch: 6, Loss:  0.1348944753408432\n",
      "Epoch: 6, Loss:  0.4201849699020386\n",
      "Epoch: 6, Loss:  0.13779610395431519\n",
      "Epoch: 6, Loss:  0.4450607895851135\n",
      "Epoch: 6, Loss:  0.13944076001644135\n",
      "Epoch: 6, Loss:  0.47350677847862244\n",
      "Epoch: 6, Loss:  0.523130476474762\n",
      "Epoch: 6, Loss:  0.8627036809921265\n",
      "Epoch: 6, Loss:  0.5049257874488831\n",
      "Epoch: 6, Loss:  0.8535011410713196\n",
      "Epoch: 6, Loss:  0.3831179440021515\n",
      "Epoch: 6, Loss:  0.8135879039764404\n",
      "Epoch: 6, Loss:  0.3439578711986542\n",
      "Epoch: 6, Loss:  0.42535555362701416\n",
      "Epoch: 6, Loss:  0.7352601885795593\n",
      "Epoch: 6, Loss:  0.39096397161483765\n",
      "Epoch: 6, Loss:  0.42622271180152893\n",
      "Epoch: 6, Loss:  0.053492482751607895\n",
      "Epoch: 6, Loss:  0.11056937277317047\n",
      "Epoch: 6, Loss:  0.6789312958717346\n",
      "Epoch: 6, Loss:  0.40354979038238525\n",
      "Epoch: 6, Loss:  0.3843816816806793\n",
      "Epoch: 6, Loss:  0.943561315536499\n",
      "Epoch: 6, Loss:  0.41612598299980164\n",
      "Epoch: 6, Loss:  0.8309112191200256\n",
      "Epoch: 6, Loss:  0.34531036019325256\n",
      "Epoch: 6, Loss:  0.4075430929660797\n",
      "Epoch: 6, Loss:  0.322806179523468\n",
      "Epoch: 7, Loss:  0.2611604332923889\n",
      "Epoch: 7, Loss:  0.14963877201080322\n",
      "Epoch: 7, Loss:  0.14060477912425995\n",
      "Epoch: 7, Loss:  0.4463950991630554\n",
      "Epoch: 7, Loss:  0.31757065653800964\n",
      "Epoch: 7, Loss:  0.41855284571647644\n",
      "Epoch: 7, Loss:  0.47816187143325806\n",
      "Epoch: 7, Loss:  0.3277268409729004\n",
      "Epoch: 7, Loss:  0.5521057844161987\n",
      "Epoch: 7, Loss:  0.44779476523399353\n",
      "Epoch: 7, Loss:  0.11260205507278442\n",
      "Epoch: 7, Loss:  0.3276723325252533\n",
      "Epoch: 7, Loss:  0.10728587210178375\n",
      "Epoch: 7, Loss:  0.5301519632339478\n",
      "Epoch: 7, Loss:  0.2901257574558258\n",
      "Epoch: 7, Loss:  0.5598530173301697\n",
      "Epoch: 7, Loss:  0.27711278200149536\n",
      "Epoch: 7, Loss:  0.37203171849250793\n",
      "Epoch: 7, Loss:  0.22085528075695038\n",
      "Epoch: 7, Loss:  0.6041157841682434\n",
      "Epoch: 7, Loss:  0.30083441734313965\n",
      "Epoch: 7, Loss:  0.39018523693084717\n",
      "Epoch: 7, Loss:  0.3553224503993988\n",
      "Epoch: 7, Loss:  0.24578966200351715\n",
      "Epoch: 7, Loss:  0.11399547755718231\n",
      "Epoch: 7, Loss:  0.16813905537128448\n",
      "Epoch: 7, Loss:  0.23243507742881775\n",
      "Epoch: 7, Loss:  0.32714778184890747\n",
      "Epoch: 7, Loss:  0.6356782913208008\n",
      "Epoch: 7, Loss:  0.4515194892883301\n",
      "Epoch: 7, Loss:  0.31219184398651123\n",
      "Epoch: 7, Loss:  0.5927510857582092\n",
      "Epoch: 7, Loss:  0.9872909188270569\n",
      "Epoch: 7, Loss:  0.2311270385980606\n",
      "Epoch: 7, Loss:  0.10293406993150711\n",
      "Epoch: 7, Loss:  0.591976523399353\n",
      "Epoch: 7, Loss:  0.4293002784252167\n",
      "Epoch: 7, Loss:  0.08523770421743393\n",
      "Epoch: 7, Loss:  0.2134183943271637\n",
      "Epoch: 7, Loss:  0.48642978072166443\n",
      "Epoch: 7, Loss:  0.06732864677906036\n",
      "Epoch: 7, Loss:  0.42019256949424744\n",
      "Epoch: 7, Loss:  0.29186421632766724\n",
      "Epoch: 7, Loss:  0.2713862657546997\n",
      "Epoch: 7, Loss:  0.3559025526046753\n",
      "Epoch: 8, Loss:  0.2340281456708908\n",
      "Epoch: 8, Loss:  0.2891745865345001\n",
      "Epoch: 8, Loss:  0.2461077868938446\n",
      "Epoch: 8, Loss:  0.12662923336029053\n",
      "Epoch: 8, Loss:  0.15105612576007843\n",
      "Epoch: 8, Loss:  0.08234436810016632\n",
      "Epoch: 8, Loss:  0.5382810831069946\n",
      "Epoch: 8, Loss:  0.1736486405134201\n",
      "Epoch: 8, Loss:  0.2138625830411911\n",
      "Epoch: 8, Loss:  0.06659979373216629\n",
      "Epoch: 8, Loss:  0.43815523386001587\n",
      "Epoch: 8, Loss:  0.31569528579711914\n",
      "Epoch: 8, Loss:  0.39988231658935547\n",
      "Epoch: 8, Loss:  0.9360310435295105\n",
      "Epoch: 8, Loss:  0.1891368329524994\n",
      "Epoch: 8, Loss:  0.5194315314292908\n",
      "Epoch: 8, Loss:  0.0930027961730957\n",
      "Epoch: 8, Loss:  0.5778788924217224\n",
      "Epoch: 8, Loss:  0.3717414438724518\n",
      "Epoch: 8, Loss:  0.2844366729259491\n",
      "Epoch: 8, Loss:  0.49485278129577637\n",
      "Epoch: 8, Loss:  0.2341058850288391\n",
      "Epoch: 8, Loss:  0.2690451145172119\n",
      "Epoch: 8, Loss:  0.17221587896347046\n",
      "Epoch: 8, Loss:  0.301437109708786\n",
      "Epoch: 8, Loss:  0.09826968610286713\n",
      "Epoch: 8, Loss:  0.3217995762825012\n",
      "Epoch: 8, Loss:  0.12723498046398163\n",
      "Epoch: 8, Loss:  0.22944867610931396\n",
      "Epoch: 8, Loss:  0.3028867244720459\n",
      "Epoch: 8, Loss:  0.20334744453430176\n",
      "Epoch: 8, Loss:  0.05486954376101494\n",
      "Epoch: 8, Loss:  0.37895479798316956\n",
      "Epoch: 8, Loss:  0.10232961922883987\n",
      "Epoch: 8, Loss:  0.27604562044143677\n",
      "Epoch: 8, Loss:  0.06449121236801147\n",
      "Epoch: 8, Loss:  0.24088752269744873\n",
      "Epoch: 8, Loss:  0.37516674399375916\n",
      "Epoch: 8, Loss:  0.22349214553833008\n",
      "Epoch: 8, Loss:  0.5102614760398865\n",
      "Epoch: 8, Loss:  0.27262264490127563\n",
      "Epoch: 8, Loss:  0.10261570662260056\n",
      "Epoch: 8, Loss:  0.04883571341633797\n",
      "Epoch: 8, Loss:  0.09658658504486084\n",
      "Epoch: 8, Loss:  0.12391103059053421\n",
      "Epoch: 9, Loss:  0.20919324457645416\n",
      "Epoch: 9, Loss:  0.07613720744848251\n",
      "Epoch: 9, Loss:  0.2520422637462616\n",
      "Epoch: 9, Loss:  0.03027389757335186\n",
      "Epoch: 9, Loss:  0.34589827060699463\n",
      "Epoch: 9, Loss:  0.12895123660564423\n",
      "Epoch: 9, Loss:  0.3850964307785034\n",
      "Epoch: 9, Loss:  0.22926759719848633\n",
      "Epoch: 9, Loss:  0.1069343313574791\n",
      "Epoch: 9, Loss:  0.11988604068756104\n",
      "Epoch: 9, Loss:  0.13159729540348053\n",
      "Epoch: 9, Loss:  0.12207122147083282\n",
      "Epoch: 9, Loss:  0.10696322470903397\n",
      "Epoch: 9, Loss:  0.15214651823043823\n",
      "Epoch: 9, Loss:  0.33513084053993225\n",
      "Epoch: 9, Loss:  0.3157132565975189\n",
      "Epoch: 9, Loss:  0.1848450005054474\n",
      "Epoch: 9, Loss:  0.06739386916160583\n",
      "Epoch: 9, Loss:  0.2165469527244568\n",
      "Epoch: 9, Loss:  0.2314472645521164\n",
      "Epoch: 9, Loss:  0.12009172141551971\n",
      "Epoch: 9, Loss:  0.10177331417798996\n",
      "Epoch: 9, Loss:  0.47163236141204834\n",
      "Epoch: 9, Loss:  0.2585430443286896\n",
      "Epoch: 9, Loss:  0.4146149158477783\n",
      "Epoch: 9, Loss:  0.14357830584049225\n",
      "Epoch: 9, Loss:  0.13936670124530792\n",
      "Epoch: 9, Loss:  0.1361546814441681\n",
      "Epoch: 9, Loss:  0.15335363149642944\n",
      "Epoch: 9, Loss:  0.25790953636169434\n",
      "Epoch: 9, Loss:  0.25842228531837463\n",
      "Epoch: 9, Loss:  0.21409524977207184\n",
      "Epoch: 9, Loss:  0.10060977935791016\n",
      "Epoch: 9, Loss:  0.6155354380607605\n",
      "Epoch: 9, Loss:  0.8569517731666565\n",
      "Epoch: 9, Loss:  0.116060771048069\n",
      "Epoch: 9, Loss:  0.24828745424747467\n",
      "Epoch: 9, Loss:  0.32493162155151367\n",
      "Epoch: 9, Loss:  0.10372209548950195\n",
      "Epoch: 9, Loss:  0.22848327457904816\n",
      "Epoch: 9, Loss:  0.11408870667219162\n",
      "Epoch: 9, Loss:  0.18309758603572845\n",
      "Epoch: 9, Loss:  0.3629610240459442\n",
      "Epoch: 9, Loss:  0.24111588299274445\n",
      "Epoch: 9, Loss:  0.5914976596832275\n"
     ]
    }
   ],
   "source": [
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "for epoch in range(TRAIN_EPOCHS):\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
      "Completed 0\n",
      "Completed 100\n",
      "Completed 200\n",
      "Completed 300\n",
      "Completed 400\n",
      "Completed 500\n",
      "Completed 600\n",
      "Completed 700\n",
      "Completed 800\n",
      "Completed 900\n",
      "Completed 1000\n",
      "Completed 1100\n",
      "Completed 1200\n",
      "Completed 1300\n",
      "Completed 1400\n",
      "Completed 1500\n",
      "Completed 1600\n",
      "Completed 1700\n",
      "Completed 1800\n",
      "Completed 1900\n",
      "Completed 2000\n",
      "Completed 2100\n",
      "Completed 2200\n",
      "Completed 2300\n",
      "Completed 2400\n",
      "Completed 2500\n",
      "Completed 2600\n",
      "Completed 2700\n",
      "Completed 2800\n",
      "Completed 2900\n",
      "Completed 3000\n",
      "Completed 3100\n",
      "Completed 3200\n",
      "Completed 3300\n",
      "Completed 3400\n",
      "Completed 3500\n",
      "Completed 3600\n",
      "Completed 3700\n",
      "Completed 3800\n",
      "Completed 3900\n",
      "Completed 4000\n",
      "Completed 4100\n",
      "Completed 4200\n",
      "Completed 4300\n",
      "Completed 4400\n",
      "Completed 4500\n",
      "Completed 4600\n",
      "Completed 4700\n",
      "Completed 4800\n",
      "Completed 4900\n",
      "Completed 5000\n",
      "Completed 5100\n",
      "Completed 5200\n",
      "Completed 5300\n",
      "Completed 5400\n",
      "Completed 5500\n"
     ]
    }
   ],
   "source": [
    "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Text</th>\n",
       "      <th>Actual Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: 3 dead, 30 injured in two blasts in Bangladesh</td>\n",
       "      <td>summarize: At least 3 killed, 30 injured in bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: 30 blasts at ordnance factory in MP</td>\n",
       "      <td>summarize: 30 blasts occur at ordnance factory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Centre rejects reports of larger Nagaland state</td>\n",
       "      <td>summarize: Govt dismisses reports of carving o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: Thousands protest against Brexit in London</td>\n",
       "      <td>summarize: Thousands march in London to protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>: Wankhede Stadium reserved for underprivilege...</td>\n",
       "      <td>summarize: Underprivileged kids to fill Wankhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>: Hanumantha Rao had also written to HRD in 2014</td>\n",
       "      <td>summarize: Congress MP wrote to HRD ministry i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>: SPG moves SC against diesel vehicles ban</td>\n",
       "      <td>summarize: SPG seeks exception in SC ban on di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11018</th>\n",
       "      <td>: Sonu Nigam surprises passengers with impromp...</td>\n",
       "      <td>summarize: Sonu Nigam sings for co-passengers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11019</th>\n",
       "      <td>: TRAI slams FB over &amp;#39;Free Basics&amp;#39; debate</td>\n",
       "      <td>summarize: TRAI slams FB for &amp;#39;orchestrated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11020</th>\n",
       "      <td>: Ghulam Ali to make acting debut in Bollywood</td>\n",
       "      <td>summarize: Ghulam Ali set to make acting debut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11021 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Generated Text  \\\n",
       "0       : 3 dead, 30 injured in two blasts in Bangladesh   \n",
       "1                  : 30 blasts at ordnance factory in MP   \n",
       "2      : Centre rejects reports of larger Nagaland state   \n",
       "3           : Thousands protest against Brexit in London   \n",
       "4      : Wankhede Stadium reserved for underprivilege...   \n",
       "...                                                  ...   \n",
       "11016   : Hanumantha Rao had also written to HRD in 2014   \n",
       "11017         : SPG moves SC against diesel vehicles ban   \n",
       "11018  : Sonu Nigam surprises passengers with impromp...   \n",
       "11019  : TRAI slams FB over &#39;Free Basics&#39; debate   \n",
       "11020     : Ghulam Ali to make acting debut in Bollywood   \n",
       "\n",
       "                                             Actual Text  \n",
       "0      summarize: At least 3 killed, 30 injured in bl...  \n",
       "1      summarize: 30 blasts occur at ordnance factory...  \n",
       "2      summarize: Govt dismisses reports of carving o...  \n",
       "3      summarize: Thousands march in London to protes...  \n",
       "4      summarize: Underprivileged kids to fill Wankhe...  \n",
       "...                                                  ...  \n",
       "11016  summarize: Congress MP wrote to HRD ministry i...  \n",
       "11017  summarize: SPG seeks exception in SC ban on di...  \n",
       "11018  summarize: Sonu Nigam sings for co-passengers ...  \n",
       "11019  summarize: TRAI slams FB for &#39;orchestrated...  \n",
       "11020  summarize: Ghulam Ali set to make acting debut...  \n",
       "\n",
       "[11021 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/t5-inshorts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['lm_head.weight', 'decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf_model = transformers.TFT5ForConditionalGeneration.from_pretrained('models/t5-inshorts/', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.save_pretrained('models/t5-inshorts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer_config.json',\n",
       " './special_tokens_map.json',\n",
       " './spiece.model',\n",
       " './added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
