{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "from captum.attr import (IntegratedGradients, LayerIntegratedGradients,\n",
    "                         configure_interpretable_embedding_layer,\n",
    "                         remove_interpretable_embedding_layer)\n",
    "from transformers import (ElectraForSequenceClassification,\n",
    "                          ElectraTokenizerFast, EvalPrediction, InputFeatures,\n",
    "                          Trainer, TrainingArguments, glue_compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer\n",
    "\n",
    "model = ElectraForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", num_labels = 2)\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\n",
    "    \"google/electra-small-discriminator\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/mirac13/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set labels: {0, 1}\n",
      "Validation set labels: {0, 1}\n",
      "Test set labels: {-1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates something rather beautiful about human nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same throughout</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     senence  \\\n",
       "0  hide new secretions from the parental units                                                 \n",
       "1  contains no wit , only labored gags                                                         \n",
       "2  that loves its characters and communicates something rather beautiful about human nature    \n",
       "3  remains utterly satisfied to remain the same throughout                                     \n",
       "4  on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up                    \n",
       "\n",
       "   label  \n",
       "0  0      \n",
       "1  0      \n",
       "2  1      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SST2 dataset from the datasets library\n",
    "dataset = datasets.load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Look at the labels\n",
    "print(\"Training set labels: {}\".format(set(dataset[\"train\"][\"label\"])))\n",
    "print(\"Validation set labels: {}\".format(set(dataset[\"validation\"][\"label\"])))\n",
    "print(\"Test set labels: {}\".format(set(dataset[\"test\"][\"label\"])))\n",
    "\n",
    "# Explore the dataset\n",
    "df = pd.DataFrame({\"senence\": dataset[\"train\"][\"sentence\"],\n",
    "                   \"label\": dataset[\"train\"][\"label\"]})\n",
    "pd.options.display.max_colwidth = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "\n",
    "class TrainerDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.tokenized_inputs = tokenizer(inputs, padding=True)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return InputFeatures(\n",
    "            input_ids=self.tokenized_inputs['input_ids'][idx],\n",
    "            token_type_ids=self.tokenized_inputs['token_type_ids'][idx],\n",
    "            attention_mask=self.tokenized_inputs['attention_mask'][idx],\n",
    "            label=self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainerDataset(dataset[\"train\"][\"sentence\"],\n",
    "                               dataset[\"train\"][\"label\"], tokenizer)\n",
    "eval_dataset = TrainerDataset(dataset[\"validation\"][\"sentence\"],\n",
    "                              dataset[\"validation\"][\"label\"], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"electra_sst2\",\n",
    "    num_train_epochs=3,  # (1 epoch gives slightly lower accuracy)\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,    \n",
    "    dataloader_drop_last=True,  # Make sure all batches are of equal size\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    # The choice of a dataset (task_name) implies metric\n",
    "    return glue_compute_metrics(\n",
    "        task_name=\"sst-2\",\n",
    "        preds=preds,\n",
    "        labels=p.label_ids)\n",
    "\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 04:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6312, training_loss=0.18832752641162945, metrics={'train_runtime': 282.3714, 'train_samples_per_second': 22.354, 'total_flos': 1083750877034496, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9185779816513762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirac13/.conda/envs/torch/lib/python3.8/site-packages/transformers/data/metrics/__init__.py:66: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/mirac13/.conda/envs/torch/lib/python3.8/site-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "model_result = trainer.evaluate()\n",
    "print(\"Accuracy: {}\".format(model_result[\"eval_acc\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "\n",
    "The examples below use two attribution methods from the Captum library:\n",
    "\n",
    "- **Integrated Gradients** - the method requires configuring interpretation hooks to perform attribution for all three embedding layers in one step, and\n",
    "- **Layer Integrated Gradients**, computed separately with respect to each of the three layers:\n",
    "     - model.electra.embeddings.word_embeddings,\n",
    "     - model.electra.embeddings.token_type_embeddings,\n",
    "     - model.electra.embeddings.position_embeddings.\n",
    "     \n",
    "We will try to find out to what extent, according to these methods, each token has contributed to the model's prediction, or, more precisely, to its shift from the baseline output. Each method requires setting a target class index: 0 for negative or 1 for a positive sentiment. Attribution is performed for each target class separately. Scores will be assigned with regard to the model's output for the selected class.\n",
    "\n",
    "The shape of attributions is the same as the shape of the inputs parameter of the attribute method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 857,\n",
       "  'label': 1,\n",
       "  'sentence': 'visually imaginative , thematically instructive and thoroughly delightful , it takes us on a roller-coaster ride from innocence to experience without even a hint of that typical kiddie-flick sentimentality . '}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"visually imaginative , thematically instructive and thoroughly \\\n",
    "delightful , it takes us on a roller-coaster ride from innocence to experience \\\n",
    "without even a hint of that typical kiddie-flick sentimentality . \"\n",
    "true_label = 1\n",
    "\n",
    "[x for x in dataset[\"validation\"] if x[\"sentence\"] == text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "The functions below construct input tensors for our sample and for a sequence of [PAD] tokens serving as a baseline. We also need to define a forward function running inference on the model. The function will be passed on to objects handling attribution.\n",
    "\n",
    "Computation with IntegratedGradients requires altering the model by configuring additional layers. For this purpose, the Captum library provides the configure_interpretable_embedding_layer and remove_interpretable_embedding_layer methods. Configuring an interpretable embedding layer modifies the model. A model with interpretable layers requires input of a different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_interpretable_embeddings():\n",
    "    \"\"\"Configure interpretable embedding layer\"\"\"\n",
    "    interpretable_embedding1 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.word_embeddings\")\n",
    "    interpretable_embedding2 = configure_interpretable_embedding_layer(\n",
    "        model, \"electra.embeddings.token_type_embeddings\")\n",
    "    interpretable_embedding3 = configure_interpretable_embedding_layer(\n",
    "        model,\"electra.embeddings.position_embeddings\")\n",
    "    return (interpretable_embedding1,\n",
    "            interpretable_embedding2,\n",
    "            interpretable_embedding3)\n",
    "\n",
    "\n",
    "def remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                    interpretable_embedding2, \n",
    "                                    interpretable_embedding3):\n",
    "    \"\"\"Remove interpretable layer to restore the original model structure\"\"\"\n",
    "    if not \\\n",
    "    type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "        return\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding1)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding2)\n",
    "    remove_interpretable_embedding_layer(model, interpretable_embedding3)  \n",
    "\n",
    "\n",
    "def predict_forward_func(input_ids, token_type_ids=None, \n",
    "                         position_ids=None, attention_mask=None):\n",
    "    \"\"\"Function passed to ig constructors\"\"\"\n",
    "    return model(input_ids=input_ids, \n",
    "                 token_type_ids=token_type_ids, \n",
    "                 position_ids=position_ids, \n",
    "                 attention_mask=attention_mask)[0]  \n",
    "\n",
    "\n",
    "def prepare_input(text):\n",
    "    \"\"\"Prepare ig attribution input: tokenize sample and baseline text.\"\"\"\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\", \n",
    "                               return_attention_mask=True)\n",
    "    seq_len = tokenized_text[\"input_ids\"].shape[1]\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # Construct the baseline (a reference sample).\n",
    "    # A sequence of [PAD] tokens of length equal to that of the processed sample\n",
    "    ref_text = tokenizer.pad_token * (seq_len - 2) # special tokens\n",
    "    tokenized_ref_text = tokenizer(ref_text, return_tensors=\"pt\") \n",
    "    ref_position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    return (tokenized_text[\"input_ids\"],\n",
    "            tokenized_text[\"token_type_ids\"], \n",
    "            position_ids,\n",
    "            tokenized_ref_text[\"input_ids\"],\n",
    "            tokenized_ref_text[\"token_type_ids\"], \n",
    "            ref_position_ids,\n",
    "            tokenized_text[\"attention_mask\"])   \n",
    "\n",
    "\n",
    "def prepare_input_embed(input_ids, token_type_ids, position_ids,\n",
    "                        ref_input_ids, ref_token_type_ids, ref_position_ids,\n",
    "                        attention_mask):\n",
    "    \"\"\"Construct input for the modified model\"\"\"\n",
    "    input_ids_embed = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_ids_embed = interpretable_embedding1.indices_to_embeddings(\n",
    "        ref_input_ids)\n",
    "    token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(\n",
    "        token_type_ids)\n",
    "    ref_token_type_ids_embed = interpretable_embedding2.indices_to_embeddings(\n",
    "        ref_token_type_ids)\n",
    "    position_ids_embed = interpretable_embedding3.indices_to_embeddings(\n",
    "        position_ids)\n",
    "    ref_position_ids_embed = interpretable_embedding3.indices_to_embeddings(\n",
    "        ref_position_ids)\n",
    "    \n",
    "    return (input_ids_embed, token_type_ids_embed, position_ids_embed, \n",
    "            ref_input_ids_embed, ref_token_type_ids_embed, \n",
    "            ref_position_ids_embed, attention_mask)\n",
    "\n",
    "\n",
    "def get_input_data(text):\n",
    "    input_data = place_on_device(*prepare_input(text))\n",
    "    input_data_embed = prepare_input_embed(*input_data)   \n",
    "    return input_data, input_data_embed \n",
    "\n",
    "\n",
    "def place_on_device(*tensors):\n",
    "    tensors_device = []\n",
    "    for t in tensors:\n",
    "        tensors_device.append(t.to(device))\n",
    "    return tuple(tensors_device)  \n",
    "\n",
    "\n",
    "def ig_attribute(ig, class_index, input_data_embed):\n",
    "    return ig.attribute(inputs=input_data_embed[0:3],\n",
    "                        baselines=input_data_embed[3:6],\n",
    "                        additional_forward_args=(input_data_embed[6]),\n",
    "                        target = class_index,\n",
    "                        return_convergence_delta=True,\n",
    "                        n_steps=200)\n",
    "    \n",
    "\n",
    "def lig_attribute(lig, class_index, input_data):\n",
    "    return lig.attribute(\n",
    "        inputs=input_data[0], baselines=input_data[3],\n",
    "        additional_forward_args=(input_data[1], input_data[2], input_data[6]),\n",
    "        return_convergence_delta=True, target=class_index, n_steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Gradients\n",
    "To compute attributions with Integrated Gradients we will:\n",
    "\n",
    "- instantiate the IntegratedGradients class passing the predict_forward_func function as parameter,\n",
    "- configure interpretable embedding layer,\n",
    "- prepare input tensors,\n",
    "- compute attributions,\n",
    "- remove interpratable embedding layer.\n",
    "- Calling the get_input_embeddings method of the model helps to find out whether extra layers have been configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the IntegratedGradients class\n",
    "ig = IntegratedGradients(predict_forward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model's input embeddings:\n",
      " Embedding(30522, 128, padding_idx=0)\n",
      "\n",
      "Input embeddings with interpretable layer:\n",
      " InterpretableEmbeddingBase(\n",
      "  (embedding): Embedding(30522, 128, padding_idx=0)\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirac13/.conda/envs/torch/lib/python3.8/site-packages/captum/attr/_models/base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input embeddings with interpretable layer removed:\n",
      " Embedding(30522, 128, padding_idx=0)\n",
      "\n",
      "\n",
      "The reference sample:\n",
      "['[CLS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Configure interpretable embedding layer \n",
    "print(\"Original model's input embeddings:\\n {}\\n\".\n",
    "      format(model.get_input_embeddings()))\n",
    "if not \\\n",
    "type(model.get_input_embeddings()).__name__ == \"InterpretableEmbeddingBase\":\n",
    "    interpretable_embedding1, interpretable_embedding2,\\\n",
    "    interpretable_embedding3 = configure_interpretable_embeddings()\n",
    "print(\"Input embeddings with interpretable layer:\\n {}\\n\".\n",
    "      format(model.get_input_embeddings()))\n",
    "\n",
    "# Prepare input\n",
    "input_data, input_data_embed = get_input_data(text)  \n",
    "\n",
    "# Compute attributions for both target classes\n",
    "# class 0 (negative)\n",
    "attributions_0, approximation_error_0 = ig_attribute(ig, 0, input_data_embed)\n",
    "# class 1 (positive)\n",
    "attributions_1, approximation_error_1 = ig_attribute(ig, 1, input_data_embed)\n",
    "\n",
    "# Remove interpratable embedding layer used by ig attribution\n",
    "remove_interpretable_embeddings(interpretable_embedding1, \n",
    "                                interpretable_embedding2, \n",
    "                                interpretable_embedding3)\n",
    "print(\"\\nInput embeddings with interpretable layer removed:\\n {}\\n\"\n",
    ".format(model.get_input_embeddings()))\n",
    "\n",
    "print(\"\\nThe reference sample:\\n{}\".format(tokenizer.convert_ids_to_tokens(\n",
    "    input_data[3].clone().detach().to('cpu').numpy().squeeze())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completeness\n",
    "The Integrated Gradients method satisfies the completeness property. The sum of attributions should be equal, with certain accuracy, to the difference between the model's output for the sample and its output for the selected baseline (in this case a sequence of [PAD] tokens). Increase the n_steps parameter of the attribute method to obtain better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: input tokens attr. sum: -2.011822774770072\n",
      "Classs 0: token type attr. sum: 0.0\n",
      "Class 0: position ids attr. sum: 0.0\n",
      "Class 1: input tokens attr. sum: 2.1013772979985452\n",
      "Classs 1: token type attr. sum: 0.0\n",
      "Class 1: position ids attr. sum: 0.0\n",
      "\n",
      "Prediction for sample: [-3.332552   3.5051985]\n",
      "Prediction for baseline: [-1.3202178  1.4033823]\n",
      "Difference from baseline: [-2.012334   2.1018162]\n",
      "Sum of attributions: [-2.011822774770072, 2.1013772979985452]\n",
      "\n",
      "Class 0:\n",
      " score: -3.332551956176758\n",
      " reference score: -1.3202178478240967    \n",
      " difference from ref.: -2.012334108352661\n",
      " sum of attributions:  -2.011822774770072    \n",
      " difference from reference - attributions: -0.0005113335825890353\n",
      "\n",
      "Class 1:\n",
      " score: 3.5051984786987305\n",
      " reference score: 1.4033823013305664    \n",
      " difference from ref.: 2.101816177368164\n",
      " sum of attributions:  2.1013772979985452    \n",
      " difference from reference - attributions: 0.00043887936961883867\n"
     ]
    }
   ],
   "source": [
    "def check_completeness(attributions_0, attributions_1):\n",
    "    input_ids, token_type_ids, position_ids, ref_input_ids,\\\n",
    "    ref_token_type_ids, ref_position_ids, attention_mask = input_data\n",
    "\n",
    "    # Prediction for the sample\n",
    "    scores = predict_forward_func(input_ids, token_type_ids,\n",
    "                                position_ids, attention_mask) \n",
    "    # Prediction for the baseline\n",
    "    ref_scores = predict_forward_func(ref_input_ids, ref_token_type_ids,\n",
    "                                    ref_position_ids, attention_mask)\n",
    "\n",
    "    # Put on cpu\n",
    "    if torch.is_tensor(attributions_0[0]):\n",
    "        attributions_0 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_0]\n",
    "    if torch.is_tensor(attributions_1[0]):\n",
    "        attributions_1 = [x.clone().detach().to('cpu').numpy() \n",
    "        for x in attributions_1]  \n",
    "    scores = scores.clone().detach().to('cpu').numpy().squeeze()\n",
    "    ref_scores = ref_scores.clone().detach().to('cpu').numpy().squeeze()    \n",
    "\n",
    "    # How prediction for the sample differs from baseline prediction  \n",
    "    diff_from_baseline = scores - ref_scores\n",
    "\n",
    "    # Sum of attributions\n",
    "    attributions_sum0 = [x.sum() for x in attributions_0]\n",
    "    attributions_sum1 = [x.sum() for x in attributions_1]\n",
    "    attributions_sum = [sum(attributions_sum0), sum(attributions_sum1)]\n",
    "\n",
    "    # Difference from the baseline output for both classes\n",
    "    diff = diff_from_baseline - attributions_sum\n",
    "\n",
    "    # Find out which layers contribute to the score \n",
    "    print(\"Class 0: input tokens attr. sum: {}\".format(attributions_sum0[0]))\n",
    "    print(\"Classs 0: token type attr. sum: {}\".format(attributions_0[1].sum()))\n",
    "    print(\"Class 0: position ids attr. sum: {}\".format(attributions_0[2].sum()))\n",
    "    print(\"Class 1: input tokens attr. sum: {}\".format(attributions_1[0].sum()))\n",
    "    print(\"Classs 1: token type attr. sum: {}\".format(attributions_1[1].sum()))\n",
    "    print(\"Class 1: position ids attr. sum: {}\".format(attributions_1[2].sum()))\n",
    "\n",
    "    # Compare sum of attributions with the difference from baseline prediction\n",
    "    print(\"\\nPrediction for sample: {}\".format(scores))\n",
    "    print(\"Prediction for baseline: {}\".format(ref_scores))\n",
    "    print(\"Difference from baseline: {}\".format(diff_from_baseline))\n",
    "    print(\"Sum of attributions: {}\".format(attributions_sum))\n",
    "    print(\"\\nClass 0:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[0], ref_scores[0], diff_from_baseline[0], \n",
    "            attributions_sum[0], diff[0]))\n",
    "    print(\"\\nClass 1:\\n score: {}\\n reference score: {}\\\n",
    "    \\n difference from ref.: {}\\n sum of attributions:  {}\\\n",
    "    \\n difference from reference - attributions: {}\".\\\n",
    "    format(scores[1], ref_scores[1], diff_from_baseline[1], \n",
    "            attributions_sum[1], diff[1]))\n",
    "    \n",
    "    return attributions_0, attributions_1\n",
    "    \n",
    "    \n",
    "attributions_0, attributions_1 = check_completeness(attributions_0,\n",
    "                                                    attributions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
